# -*- coding: utf-8 -*-
"""Huggingface_Albert_f.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/158UWVgEYInqYQR1FgnjJOD4dlGHpAnsi
"""


import json
from pathlib import Path

def read_squad(path):
    path = Path(path)
    with open(path, 'rb') as f:
        squad_dict = json.load(f)

    contexts = []
    questions = []
    answers = []
    ids = []
    for group in squad_dict['data']:
        for passage in group['paragraphs']:
            context = passage['context']
            for qa in passage['qas']:
                question = qa['question']
                id = qa['id']
                for answer in qa['answers']:
                    contexts.append(context)
                    questions.append(question)
                    answers.append(answer)
                    ids.append(id)
    return contexts, questions, answers, ids

train_contexts, train_questions, train_answers, train_ids = read_squad('squad/train-v2.0.json')
val_contexts, val_questions, val_answers, val_ids = read_squad('squad/dev-v2.0.json')


val_contexts = val_contexts[0:20288]
val_questions = val_questions[0:20288]  
val_answers = val_answers[0:20288]

def add_end_idx(answers, contexts):
    for answer, context in zip(answers, contexts):
        gold_text = answer['text']
        start_idx = answer['answer_start']
        end_idx = start_idx + len(gold_text)

        # sometimes squad answers are off by a character or two â€“ fix this
        if context[start_idx:end_idx] == gold_text:
            answer['answer_end'] = end_idx
        elif context[start_idx-1:end_idx-1] == gold_text:
            answer['answer_start'] = start_idx - 1
            answer['answer_end'] = end_idx - 1     # When the gold label is off by one character
        elif context[start_idx-2:end_idx-2] == gold_text:
            answer['answer_start'] = start_idx - 2
            answer['answer_end'] = end_idx - 2     # When the gold label is off by two characters

add_end_idx(train_answers, train_contexts)
add_end_idx(val_answers, val_contexts)

from transformers import AlbertTokenizerFast
tokenizer = AlbertTokenizerFast.from_pretrained('albert-base-v2')

train_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True)
val_encodings = tokenizer(val_contexts, val_questions, truncation=True, padding=True)


def add_token_positions(encodings, answers):
    start_positions = []
    end_positions = []
    for i in range(len(answers)):
        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))
        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end'] - 1))

        # if start position is None, the answer passage has been truncated
        if start_positions[-1] is None:
            start_positions[-1] = tokenizer.model_max_length
        if end_positions[-1] is None:
            end_positions[-1] = tokenizer.model_max_length

    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})

add_token_positions(train_encodings, train_answers)
add_token_positions(val_encodings, val_answers)

import torch

class SquadDataset(torch.utils.data.Dataset):
    def __init__(self, encodings):
        self.encodings = encodings

    def __getitem__(self, idx):
        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}

    def __len__(self):
        return len(self.encodings.input_ids)

train_dataset = SquadDataset(train_encodings)
val_dataset = SquadDataset(val_encodings)

from transformers import AlbertForQuestionAnswering
model = AlbertForQuestionAnswering.from_pretrained("albert-base-v2")


from tqdm.notebook import tqdm, trange
from torch.utils.data import DataLoader
from transformers import AdamW

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')

model.to(device)
model.train()

train_loader = DataLoader(train_dataset, batch_size=3, shuffle=True)

optim = AdamW(model.parameters(), lr=5e-5)

for epoch in trange(3, desc='Overall Progress'):
    for batch in tqdm(train_loader, desc='Batch Progess'):
        optim.zero_grad()
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        start_positions = batch['start_positions'].to(device)
        end_positions = batch['end_positions'].to(device)
        token_type_ids = batch['token_type_ids'].to(device)
        outputs = model(input_ids, attention_mask = attention_mask, token_type_ids = token_type_ids, start_positions=start_positions, end_positions=end_positions)
        loss = outputs[0]
        loss.backward()
        optim.step()

model.eval()

eval_loader = DataLoader(val_dataset, batch_size=16, shuffle=True)


from datasets import load_metric
metric = load_metric('squad_v2')

context_idx = 0
for batch in tqdm(eval_loader, desc='Evaluation Progess'):
    batch = {k: v.to(device) for k, v in batch.items()}
    with torch.no_grad():
        outputs = model(**batch)

    answer_start_scores = outputs.start_logits
    answer_end_scores = outputs.end_logits
    start_score_prediction = torch.argmax(answer_start_scores, dim=-1)
    end_score_prediction = torch.argmax(answer_end_scores, dim=-1)


    predictions_all = []
    references_all = []

    for i in range(0, 3):
        tokens = tokenizer.convert_ids_to_tokens(batch['input_ids'][i])
        
        #Predicted Answers
        answer_start = start_score_prediction[i]
        answer_end = end_score_prediction[i]
        answer = tokens[answer_start]
        for j in range(answer_start + 1, answer_end + 1):
            if tokens[j][0:2] == '##':
                answer += tokens[j][2:]
            else:
                answer += ' ' + tokens[j]
                
        #Correct Answers
        
        correct_start = batch['start_positions'][i]
        correct_end = batch['end_positions'][i]
        if(correct_start >= len(tokens)):
            continue
        correct_answer = tokens[correct_start]
        
        
        
        for j in range(correct_start + 1, correct_end + 1):
            if tokens[j][0:2] == '##':
                correct_answer += tokens[j][2:]
            else:
                correct_answer += ' ' + tokens[j]
        
        predictions = {}
        predictions['id'] = str(context_idx)
        predictions['no_answer_probability'] = 0
        predictions['prediction_text'] = answer

        references = {}
        references['id'] = str(context_idx)
        references['answers'] = []

        temp_dict = {}
        temp_dict['answer_start'] = correct_start.tolist()
        temp_dict['text'] = correct_answer

        references['answers'].append(temp_dict)

        predictions_all.append(predictions)
        references_all.append(references)
        context_idx += 1

    metric.add_batch(predictions=predictions_all, references=references_all)

print(metric.compute())


